## 实验一 Linear Regression
#### 实验目的
学习线性回归，使用梯度下降算法解决线性回归问题
#### 实验环境
python3 + jupyter nookbook
#### 实验思路
根据梯度下降算法的数学原理：
$$ Loss = (wX + b -Y)^2$$

$$ \nabla Loss(w) = \frac{d}{dw}Loss(w) =  2(X^TXw+b - X^TY)$$

$$ \nabla Loss(b) = \frac{d}{db}Loss(b) = 2(Xw+b - Y)$$

$$w_{new} = w_{old}+\eta\nabla Loss(w_{old}),\ \ b_{new} = b_{old}+\eta\nabla Loss(b_{old})$$
推导编写函数进行梯度计算与权重更新。
#### 实验步骤
使用pandas读入提供数据并转换为numpy数组，根据数学原理编写梯度计算函数与更新权重参数，按照要求绘制数据图像。权重w、偏移量b均初始化为1，学习率取0.0007，更新15000次，得到结果。

#### 实验结果
第一次更新的结果：w = 0.9993 b = 0.9993
最终结果：w = 0.0703999999999819, b = 0.7185999999999864

#### 体会与总结
最开始使用实验手册的**学习率**0.007进行求解，发现过大，我编写的梯度下降函数无法得到较好的解，将学习率再下降一个数量级得到了更好的解。学习率、初始化值的选择都对梯度下降算法的最终效果有很大的影响。
#### 代码
```python
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
```


```python
path = os.path.join(os.getcwd(),'ex1Data/')
# 使用科学计数法读取浮点数
dfx = pd.read_csv(path+'ex1x.dat', delimiter='\n', header=None, dtype=float, names=['col1'])
dfy = pd.read_csv(path+'ex1y.dat', delimiter='\n', header=None, dtype=float, names=['col1'])
# 将DataFrame转换为一维NumPy数组
datax = dfx['col1'].values
datay = dfy['col1'].values
```


```python
plt.xlabel("Age in years")
plt.ylabel("Height in meters")
plt.scatter(datax,datay, alpha=0.4)
```


```python
def nabla_loss(w, b, X, Y):#计算梯度
    return (2*(np.dot(X.T, np.dot(X, w)+b) - np.dot(X.T, Y))/len(Y),
            2*np.mean(np.dot(X, w)+b - Y))
def gd(w, b, X, Y, eta, epoch):
    for i in range(epoch):
        grad_w, grad_b = nabla_loss(w, b, X, Y) 
        w -= eta * grad_w / np.linalg.norm(grad_w)#更新
        b -= eta * grad_b / np.linalg.norm(grad_b)
        if i == 0:
            print(w, b)
    return w, b
```


```python
w, b = gd(1, 1, datax, datay, eta = 0.0007, epoch = 15000)
print(w, b)
```

    0.9993 0.9993
    0.0703999999999819 0.7185999999999864



```python
plt.xlabel("Age in years")
plt.ylabel("Height in meters")
plt.scatter(datax,datay, alpha=0.4)
plt.plot([2,8],[b,8*w+b])
```

    



```python
vw = np.linspace(-1, 1, 50)
vb = np.linspace(-1, 1, 50)
W, B = np.meshgrid(vw, vb)
Loss = (W*datax + B - datay)**2
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(W, B, Loss, cmap='rainbow')
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('loss')
```




    Text(0.5, 0, 'loss')
    

